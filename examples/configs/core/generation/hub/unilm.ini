[core/cli]
task_name = core/task/supervised_task
from_ckpt_dir = ./cache
cache_dir = ./cache

# model
[core/model/generation/unilm]
pretrained_name = unilm-base-uncased
no_repeat_ngram_size = 0
max_gen_seq_length = 15

# dataset
[core/dataset/ast]
# data columns: id, num, query, doc, label, score
data_name = fuliucansheng/mininlp

[core/dataset/ast/train]
preprocess_functions = ['core/process/unilm_generation(query, doc)']

[core/dataset/ast/dev]
preprocess_functions = ['core/process/unilm_inference(query)', 'core/process/unilm_evaluation(doc)']

[core/dataset/ast/test]
preprocess_functions = ['core/process/unilm_inference(query)']

# process
[core/process/unilm]
pretrained_name = unilm-base-uncased
max_seq_length = 24
max_gen_seq_length = 15

# task
[core/task/supervised_task]
model = core/model/generation/unilm
optim = core/optim/adamw
dataset = core/dataset/ast
loss_fn = core/loss/lm
score_fn = core/score/bleu
monitor_fns = ['core/score/bleu', 'core/score/rouge1', 'core/score/rouge2', 'core/score/rougel']
output_header = ['query', 'doc']
post_process_fn = core/postprocess/unilm_detokenize
writer_fn = core/writer/csv

from_ckpt_dir = ${core/cli:from_ckpt_dir}
to_ckpt_dir = ${core/cli:cache_dir}
output_path = ${core/cli:cache_dir}/output.txt
train_batch_size = 64
dev_batch_size = 64
test_batch_size = 256
